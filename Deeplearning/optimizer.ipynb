{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Linear:\n",
    "    def __init__(self, input_node, output_node):\n",
    "        self.weights = np.random.rand(input_node, output_node) * 0.01\n",
    "        self.bias = np.zeros((1, output_node))\n",
    "        self.grad_weights = None\n",
    "        self.grad_bias = None\n",
    "        self.mat = None\n",
    "\n",
    "        # Adagrad ÏÉÅÌÉú Î≥ÄÏàò Ï¥àÍ∏∞Ìôî\n",
    "        self.grad_squared_w = None\n",
    "        self.grad_squared_b = None\n",
    "\n",
    "    def feedforward(self, mat):\n",
    "        self.mat = mat\n",
    "        return np.dot(mat, self.weights) + self.bias\n",
    "\n",
    "    def backward(self, grad_output, clip_value=1.0):\n",
    "        self.grad_weights = np.dot(self.mat.T, grad_output)\n",
    "        self.grad_bias = np.sum(grad_output, axis=0)\n",
    "\n",
    "        grad_norm = np.linalg.norm(self.grad_weights)\n",
    "        if grad_norm > clip_value:\n",
    "            self.grad_weights *= (clip_value / grad_norm)\n",
    "            \n",
    "        return np.dot(grad_output, self.weights.T)\n",
    "\n",
    "    def update(self, optimizer):\n",
    "        optimizer.update(self)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Momentum:\n",
    "    def __init__(self, lr=0.01, beta=0.9):\n",
    "        self.lr = lr\n",
    "        self.beta = beta\n",
    "\n",
    "    def update(self, layer):\n",
    "        if not hasattr(layer, 'velocity_w'):\n",
    "            layer.velocity_w = np.zeros_like(layer.weights)\n",
    "            layer.velocity_b = np.zeros_like(layer.bias)\n",
    "\n",
    "        layer.velocity_w = self.beta * layer.velocity_w + (1 - self.beta) * layer.grad_weights\n",
    "        layer.velocity_b = self.beta * layer.velocity_b + (1 - self.beta) * layer.grad_bias\n",
    "\n",
    "        layer.weights -= self.lr * layer.velocity_w\n",
    "        layer.bias -= self.lr * layer.velocity_b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSProp:\n",
    "    def __init__(self, lr=0.001, rho=0.9, epsilon=1e-8):\n",
    "        self.lr = lr\n",
    "        self.rho = rho\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def update(self, layer):\n",
    "        if not hasattr(layer, 'rms_w'):\n",
    "            layer.rms_w = np.zeros_like(layer.weights)\n",
    "            layer.rms_b = np.zeros_like(layer.bias)\n",
    "\n",
    "        layer.rms_w = self.rho * layer.rms_w + (1 - self.rho) * (layer.grad_weights ** 2)\n",
    "        layer.rms_b = self.rho * layer.rms_b + (1 - self.rho) * (layer.grad_bias ** 2)\n",
    "\n",
    "        layer.weights -= self.lr * layer.grad_weights / (np.sqrt(layer.rms_w) + self.epsilon)\n",
    "        layer.bias -= self.lr * layer.grad_bias / (np.sqrt(layer.rms_b) + self.epsilon)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam:\n",
    "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def update(self, layer):\n",
    "        if not hasattr(layer, 'm_w'):\n",
    "            layer.m_w = np.zeros_like(layer.weights)\n",
    "            layer.v_w = np.zeros_like(layer.weights)\n",
    "            layer.m_b = np.zeros_like(layer.bias)\n",
    "            layer.v_b = np.zeros_like(layer.bias)\n",
    "            layer.t = 0\n",
    "\n",
    "        layer.t += 1\n",
    "\n",
    "        layer.m_w = self.beta1 * layer.m_w + (1 - self.beta1) * layer.grad_weights\n",
    "        layer.v_w = self.beta2 * layer.v_w + (1 - self.beta2) * (layer.grad_weights ** 2)\n",
    "\n",
    "        layer.m_b = self.beta1 * layer.m_b + (1 - self.beta1) * layer.grad_bias\n",
    "        layer.v_b = self.beta2 * layer.v_b + (1 - self.beta2) * (layer.grad_bias ** 2)\n",
    "\n",
    "        m_w_corr = layer.m_w / (1 - self.beta1 ** layer.t)\n",
    "        v_w_corr = layer.v_w / (1 - self.beta2 ** layer.t)\n",
    "        m_b_corr = layer.m_b / (1 - self.beta1 ** layer.t)\n",
    "        v_b_corr = layer.v_b / (1 - self.beta2 ** layer.t)\n",
    "\n",
    "        layer.weights -= self.lr * m_w_corr / (np.sqrt(v_w_corr) + self.epsilon)\n",
    "        layer.bias -= self.lr * m_b_corr / (np.sqrt(v_b_corr) + self.epsilon)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "\n",
    "    def update(self, layer):\n",
    "        layer.weights -= self.lr * layer.grad_weights\n",
    "        layer.bias -= self.lr * layer.grad_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adagrad:\n",
    "    def __init__(self, lr=0.01, epsilon=1e-8):\n",
    "        self.lr = lr\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def update(self, layer):\n",
    "        # Í∞Å Î†àÏù¥Ïñ¥Î≥ÑÎ°ú ÏÉÅÌÉú Ï†ÄÏû•Ïù¥ ÎêòÏñ¥ ÏûàÎäîÏßÄ ÌôïÏù∏\n",
    "        if layer.grad_squared_w is None:\n",
    "            layer.grad_squared_w = np.zeros_like(layer.weights)\n",
    "            layer.grad_squared_b = np.zeros_like(layer.bias)\n",
    "\n",
    "        # Í∞Å Î†àÏù¥Ïñ¥Ïóê ÎßûÎäî ÏÉÅÌÉúÍ∞í ÏóÖÎç∞Ïù¥Ìä∏\n",
    "        layer.grad_squared_w += layer.grad_weights ** 2\n",
    "        layer.grad_squared_b += layer.grad_bias ** 2\n",
    "\n",
    "        # ÏóÖÎç∞Ïù¥Ìä∏\n",
    "        layer.weights -= self.lr * layer.grad_weights / (np.sqrt(layer.grad_squared_w) + self.epsilon)\n",
    "        layer.bias -= self.lr * layer.grad_bias / (np.sqrt(layer.grad_squared_b) + self.epsilon)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSE:\n",
    "    @staticmethod\n",
    "    def loss(y_true, y_pred):\n",
    "        return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "    @staticmethod\n",
    "    def gradient(y_true, y_pred):\n",
    "        return 2 * (y_pred - y_true) / y_true.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, layers, optimizer):\n",
    "        self.layers = layers\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.feedforward(x)\n",
    "        return x\n",
    "\n",
    "    def backward(self, y_true, y_pred):\n",
    "        grad = MSE.gradient(y_true, y_pred)\n",
    "        for layer in reversed(self.layers):\n",
    "            grad = layer.backward(grad)\n",
    "\n",
    "    def update_weights(self):\n",
    "        for layer in self.layers:\n",
    "            if hasattr(layer, 'update'):\n",
    "                layer.update(self.optimizer)\n",
    "\n",
    "    def train(self, x_train, y_train, epochs=100, batch_size=32, shuffle=False):\n",
    "        n_samples = x_train.shape[0]\n",
    "        for epoch in range(epochs):\n",
    "            # üëâ shuffle ÏòµÏÖòÏù¥ TrueÏù∏ Í≤ΩÏö∞ÏóêÎßå ÏÑûÏñ¥Ï§ÄÎã§\n",
    "            if shuffle:\n",
    "                indices = np.arange(n_samples)\n",
    "                np.random.shuffle(indices)\n",
    "                x_train = x_train[indices]\n",
    "                y_train = y_train[indices]\n",
    "\n",
    "            for start_idx in range(0, n_samples, batch_size):\n",
    "                end_idx = start_idx + batch_size\n",
    "                x_batch = x_train[start_idx:end_idx]\n",
    "                y_batch = y_train[start_idx:end_idx]\n",
    "\n",
    "                # Forward\n",
    "                y_pred = self.forward(x_batch)\n",
    "                # Backward\n",
    "                self.backward(y_batch, y_pred)\n",
    "                # Update\n",
    "                self.update_weights()\n",
    "\n",
    "            # Optional: EpochÎ≥Ñ loss Ï∂úÎ†•\n",
    "            y_pred_full = self.forward(x_train)\n",
    "            loss = MSE.loss(y_train, y_pred_full)\n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"Epoch {epoch}, Loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "\n",
    "    def feedforward(self, x):\n",
    "        self.input = x\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        return grad_output * (self.input > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.8006\n",
      "Epoch 10, Loss: 0.0006\n",
      "Epoch 20, Loss: 0.0000\n",
      "Epoch 30, Loss: 0.0000\n",
      "Epoch 40, Loss: 0.0000\n",
      "Epoch 50, Loss: 0.0000\n",
      "Epoch 60, Loss: 0.0000\n",
      "Epoch 70, Loss: 0.0000\n",
      "Epoch 80, Loss: 0.0000\n",
      "Epoch 90, Loss: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Îç∞Ïù¥ÌÑ∞ Ï§ÄÎπÑ\n",
    "x_train = np.random.rand(500, 2)\n",
    "y_train = np.dot(x_train, np.array([[2.0], [3.0]])) + 1.0\n",
    "\n",
    "# Î™®Îç∏ Íµ¨ÏÑ±\n",
    "model = NeuralNetwork(\n",
    "    layers=[\n",
    "        Linear(2, 4),\n",
    "        ReLU(),\n",
    "        Linear(4, 1)\n",
    "    ],\n",
    "    optimizer=Adam(lr=0.1)  # SGD(lr=0.1) Î°ú Î∞îÍøîÎèÑ Í∞ÄÎä•\n",
    ")\n",
    "\n",
    "# ÌïôÏäµ (Î∞∞Ïπò ÏÇ¨Ïù¥Ï¶à 64Î°ú)\n",
    "model.train(x_train, y_train, epochs=100, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.0596\n",
      "Epoch 10, Loss: 0.0231\n",
      "Epoch 20, Loss: 0.0219\n",
      "Epoch 30, Loss: 0.0216\n",
      "Epoch 40, Loss: 0.0206\n",
      "Epoch 50, Loss: 0.0207\n",
      "Epoch 60, Loss: 0.0200\n",
      "Epoch 70, Loss: 0.0205\n",
      "Epoch 80, Loss: 0.0198\n",
      "Epoch 90, Loss: 0.0205\n",
      "Epoch 100, Loss: 0.0201\n",
      "Epoch 110, Loss: 0.0190\n",
      "Epoch 120, Loss: 0.0228\n",
      "Epoch 130, Loss: 0.0218\n",
      "Epoch 140, Loss: 0.0186\n",
      "Epoch 150, Loss: 0.0176\n",
      "Epoch 160, Loss: 0.0174\n",
      "Epoch 170, Loss: 0.0164\n",
      "Epoch 180, Loss: 0.0164\n",
      "Epoch 190, Loss: 0.0159\n",
      "Epoch 200, Loss: 0.0154\n",
      "Epoch 210, Loss: 0.0155\n",
      "Epoch 220, Loss: 0.0153\n",
      "Epoch 230, Loss: 0.0144\n",
      "Epoch 240, Loss: 0.0151\n",
      "Epoch 250, Loss: 0.0143\n",
      "Epoch 260, Loss: 0.0148\n",
      "Epoch 270, Loss: 0.0148\n",
      "Epoch 280, Loss: 0.0148\n",
      "Epoch 290, Loss: 0.0144\n",
      "Epoch 300, Loss: 0.0145\n",
      "Epoch 310, Loss: 0.0137\n",
      "Epoch 320, Loss: 0.0128\n",
      "Epoch 330, Loss: 0.0149\n",
      "Epoch 340, Loss: 0.0139\n",
      "Epoch 350, Loss: 0.0127\n",
      "Epoch 360, Loss: 0.0143\n",
      "Epoch 370, Loss: 0.0130\n",
      "Epoch 380, Loss: 0.0124\n",
      "Epoch 390, Loss: 0.0119\n",
      "Epoch 400, Loss: 0.0124\n",
      "Epoch 410, Loss: 0.0126\n",
      "Epoch 420, Loss: 0.0124\n",
      "Epoch 430, Loss: 0.0126\n",
      "Epoch 440, Loss: 0.0121\n",
      "Epoch 450, Loss: 0.0121\n",
      "Epoch 460, Loss: 0.0118\n",
      "Epoch 470, Loss: 0.0114\n",
      "Epoch 480, Loss: 0.0126\n",
      "Epoch 490, Loss: 0.0114\n",
      "Epoch 500, Loss: 0.0117\n",
      "Epoch 510, Loss: 0.0116\n",
      "Epoch 520, Loss: 0.0122\n",
      "Epoch 530, Loss: 0.0129\n",
      "Epoch 540, Loss: 0.0116\n",
      "Epoch 550, Loss: 0.0114\n",
      "Epoch 560, Loss: 0.0139\n",
      "Epoch 570, Loss: 0.0125\n",
      "Epoch 580, Loss: 0.0115\n",
      "Epoch 590, Loss: 0.0116\n",
      "Epoch 600, Loss: 0.0114\n",
      "Epoch 610, Loss: 0.0124\n",
      "Epoch 620, Loss: 0.0112\n",
      "Epoch 630, Loss: 0.0126\n",
      "Epoch 640, Loss: 0.0114\n",
      "Epoch 650, Loss: 0.0115\n",
      "Epoch 660, Loss: 0.0108\n",
      "Epoch 670, Loss: 0.0115\n",
      "Epoch 680, Loss: 0.0125\n",
      "Epoch 690, Loss: 0.0109\n",
      "Epoch 700, Loss: 0.0114\n",
      "Epoch 710, Loss: 0.0112\n",
      "Epoch 720, Loss: 0.0112\n",
      "Epoch 730, Loss: 0.0110\n",
      "Epoch 740, Loss: 0.0110\n",
      "Epoch 750, Loss: 0.0126\n",
      "Epoch 760, Loss: 0.0111\n",
      "Epoch 770, Loss: 0.0129\n",
      "Epoch 780, Loss: 0.0118\n",
      "Epoch 790, Loss: 0.0115\n",
      "Epoch 800, Loss: 0.0115\n",
      "Epoch 810, Loss: 0.0105\n",
      "Epoch 820, Loss: 0.0105\n",
      "Epoch 830, Loss: 0.0107\n",
      "Epoch 840, Loss: 0.0105\n",
      "Epoch 850, Loss: 0.0105\n",
      "Epoch 860, Loss: 0.0111\n",
      "Epoch 870, Loss: 0.0111\n",
      "Epoch 880, Loss: 0.0144\n",
      "Epoch 890, Loss: 0.0107\n",
      "Epoch 900, Loss: 0.0119\n",
      "Epoch 910, Loss: 0.0110\n",
      "Epoch 920, Loss: 0.0116\n",
      "Epoch 930, Loss: 0.0109\n",
      "Epoch 940, Loss: 0.0107\n",
      "Epoch 950, Loss: 0.0114\n",
      "Epoch 960, Loss: 0.0105\n",
      "Epoch 970, Loss: 0.0108\n",
      "Epoch 980, Loss: 0.0106\n",
      "Epoch 990, Loss: 0.0125\n",
      "Epoch 1000, Loss: 0.0112\n",
      "Epoch 1010, Loss: 0.0103\n",
      "Epoch 1020, Loss: 0.0104\n",
      "Epoch 1030, Loss: 0.0108\n",
      "Epoch 1040, Loss: 0.0104\n",
      "Epoch 1050, Loss: 0.0122\n",
      "Epoch 1060, Loss: 0.0103\n",
      "Epoch 1070, Loss: 0.0113\n",
      "Epoch 1080, Loss: 0.0103\n",
      "Epoch 1090, Loss: 0.0103\n",
      "Epoch 1100, Loss: 0.0145\n",
      "Epoch 1110, Loss: 0.0100\n",
      "Epoch 1120, Loss: 0.0105\n",
      "Epoch 1130, Loss: 0.0106\n",
      "Epoch 1140, Loss: 0.0108\n",
      "Epoch 1150, Loss: 0.0112\n",
      "Epoch 1160, Loss: 0.0113\n",
      "Epoch 1170, Loss: 0.0105\n",
      "Epoch 1180, Loss: 0.0111\n",
      "Epoch 1190, Loss: 0.0103\n",
      "Epoch 1200, Loss: 0.0104\n",
      "Epoch 1210, Loss: 0.0136\n",
      "Epoch 1220, Loss: 0.0104\n",
      "Epoch 1230, Loss: 0.0099\n",
      "Epoch 1240, Loss: 0.0119\n",
      "Epoch 1250, Loss: 0.0114\n",
      "Epoch 1260, Loss: 0.0107\n",
      "Epoch 1270, Loss: 0.0110\n",
      "Epoch 1280, Loss: 0.0109\n",
      "Epoch 1290, Loss: 0.0118\n",
      "Epoch 1300, Loss: 0.0100\n",
      "Epoch 1310, Loss: 0.0105\n",
      "Epoch 1320, Loss: 0.0108\n",
      "Epoch 1330, Loss: 0.0101\n",
      "Epoch 1340, Loss: 0.0106\n",
      "Epoch 1350, Loss: 0.0104\n",
      "Epoch 1360, Loss: 0.0105\n",
      "Epoch 1370, Loss: 0.0103\n",
      "Epoch 1380, Loss: 0.0114\n",
      "Epoch 1390, Loss: 0.0103\n",
      "Epoch 1400, Loss: 0.0101\n",
      "Epoch 1410, Loss: 0.0108\n",
      "Epoch 1420, Loss: 0.0100\n",
      "Epoch 1430, Loss: 0.0114\n",
      "Epoch 1440, Loss: 0.0109\n",
      "Epoch 1450, Loss: 0.0097\n",
      "Epoch 1460, Loss: 0.0097\n",
      "Epoch 1470, Loss: 0.0098\n",
      "Epoch 1480, Loss: 0.0098\n",
      "Epoch 1490, Loss: 0.0108\n",
      "Epoch 1500, Loss: 0.0098\n",
      "Epoch 1510, Loss: 0.0118\n",
      "Epoch 1520, Loss: 0.0099\n",
      "Epoch 1530, Loss: 0.0107\n",
      "Epoch 1540, Loss: 0.0110\n",
      "Epoch 1550, Loss: 0.0109\n",
      "Epoch 1560, Loss: 0.0101\n",
      "Epoch 1570, Loss: 0.0101\n",
      "Epoch 1580, Loss: 0.0097\n",
      "Epoch 1590, Loss: 0.0100\n",
      "Epoch 1600, Loss: 0.0108\n",
      "Epoch 1610, Loss: 0.0099\n",
      "Epoch 1620, Loss: 0.0099\n",
      "Epoch 1630, Loss: 0.0104\n",
      "Epoch 1640, Loss: 0.0104\n",
      "Epoch 1650, Loss: 0.0107\n",
      "Epoch 1660, Loss: 0.0099\n",
      "Epoch 1670, Loss: 0.0107\n",
      "Epoch 1680, Loss: 0.0103\n",
      "Epoch 1690, Loss: 0.0098\n",
      "Epoch 1700, Loss: 0.0104\n",
      "Epoch 1710, Loss: 0.0101\n",
      "Epoch 1720, Loss: 0.0096\n",
      "Epoch 1730, Loss: 0.0118\n",
      "Epoch 1740, Loss: 0.0106\n",
      "Epoch 1750, Loss: 0.0106\n",
      "Epoch 1760, Loss: 0.0099\n",
      "Epoch 1770, Loss: 0.0104\n",
      "Epoch 1780, Loss: 0.0113\n",
      "Epoch 1790, Loss: 0.0113\n",
      "Epoch 1800, Loss: 0.0105\n",
      "Epoch 1810, Loss: 0.0100\n",
      "Epoch 1820, Loss: 0.0098\n",
      "Epoch 1830, Loss: 0.0105\n",
      "Epoch 1840, Loss: 0.0101\n",
      "Epoch 1850, Loss: 0.0100\n",
      "Epoch 1860, Loss: 0.0105\n",
      "Epoch 1870, Loss: 0.0095\n",
      "Epoch 1880, Loss: 0.0100\n",
      "Epoch 1890, Loss: 0.0099\n",
      "Epoch 1900, Loss: 0.0131\n",
      "Epoch 1910, Loss: 0.0101\n",
      "Epoch 1920, Loss: 0.0105\n",
      "Epoch 1930, Loss: 0.0111\n",
      "Epoch 1940, Loss: 0.0103\n",
      "Epoch 1950, Loss: 0.0103\n",
      "Epoch 1960, Loss: 0.0100\n",
      "Epoch 1970, Loss: 0.0096\n",
      "Epoch 1980, Loss: 0.0117\n",
      "Epoch 1990, Loss: 0.0095\n",
      "Epoch 2000, Loss: 0.0110\n",
      "Epoch 2010, Loss: 0.0095\n",
      "Epoch 2020, Loss: 0.0109\n",
      "Epoch 2030, Loss: 0.0097\n",
      "Epoch 2040, Loss: 0.0106\n",
      "Epoch 2050, Loss: 0.0098\n",
      "Epoch 2060, Loss: 0.0090\n",
      "Epoch 2070, Loss: 0.0099\n",
      "Epoch 2080, Loss: 0.0092\n",
      "Epoch 2090, Loss: 0.0106\n",
      "Epoch 2100, Loss: 0.0094\n",
      "Epoch 2110, Loss: 0.0108\n",
      "Epoch 2120, Loss: 0.0104\n",
      "Epoch 2130, Loss: 0.0093\n",
      "Epoch 2140, Loss: 0.0107\n",
      "Epoch 2150, Loss: 0.0096\n",
      "Epoch 2160, Loss: 0.0095\n",
      "Epoch 2170, Loss: 0.0101\n",
      "Epoch 2180, Loss: 0.0132\n",
      "Epoch 2190, Loss: 0.0097\n",
      "Epoch 2200, Loss: 0.0098\n",
      "Epoch 2210, Loss: 0.0101\n",
      "Epoch 2220, Loss: 0.0096\n",
      "Epoch 2230, Loss: 0.0095\n",
      "Epoch 2240, Loss: 0.0090\n",
      "Epoch 2250, Loss: 0.0101\n",
      "Epoch 2260, Loss: 0.0099\n",
      "Epoch 2270, Loss: 0.0095\n",
      "Epoch 2280, Loss: 0.0107\n",
      "Epoch 2290, Loss: 0.0101\n",
      "Epoch 2300, Loss: 0.0093\n",
      "Epoch 2310, Loss: 0.0093\n",
      "Epoch 2320, Loss: 0.0098\n",
      "Epoch 2330, Loss: 0.0090\n",
      "Epoch 2340, Loss: 0.0090\n",
      "Epoch 2350, Loss: 0.0092\n",
      "Epoch 2360, Loss: 0.0094\n",
      "Epoch 2370, Loss: 0.0090\n",
      "Epoch 2380, Loss: 0.0090\n",
      "Epoch 2390, Loss: 0.0101\n",
      "Epoch 2400, Loss: 0.0089\n",
      "Epoch 2410, Loss: 0.0095\n",
      "Epoch 2420, Loss: 0.0091\n",
      "Epoch 2430, Loss: 0.0094\n",
      "Epoch 2440, Loss: 0.0092\n",
      "Epoch 2450, Loss: 0.0102\n",
      "Epoch 2460, Loss: 0.0093\n",
      "Epoch 2470, Loss: 0.0100\n",
      "Epoch 2480, Loss: 0.0092\n",
      "Epoch 2490, Loss: 0.0106\n",
      "Epoch 2500, Loss: 0.0097\n",
      "Epoch 2510, Loss: 0.0093\n",
      "Epoch 2520, Loss: 0.0096\n",
      "Epoch 2530, Loss: 0.0092\n",
      "Epoch 2540, Loss: 0.0101\n",
      "Epoch 2550, Loss: 0.0096\n",
      "Epoch 2560, Loss: 0.0093\n",
      "Epoch 2570, Loss: 0.0094\n",
      "Epoch 2580, Loss: 0.0098\n",
      "Epoch 2590, Loss: 0.0098\n",
      "Epoch 2600, Loss: 0.0093\n",
      "Epoch 2610, Loss: 0.0100\n",
      "Epoch 2620, Loss: 0.0093\n",
      "Epoch 2630, Loss: 0.0099\n",
      "Epoch 2640, Loss: 0.0094\n",
      "Epoch 2650, Loss: 0.0098\n",
      "Epoch 2660, Loss: 0.0091\n",
      "Epoch 2670, Loss: 0.0093\n",
      "Epoch 2680, Loss: 0.0098\n",
      "Epoch 2690, Loss: 0.0086\n",
      "Epoch 2700, Loss: 0.0106\n",
      "Epoch 2710, Loss: 0.0091\n",
      "Epoch 2720, Loss: 0.0094\n",
      "Epoch 2730, Loss: 0.0104\n",
      "Epoch 2740, Loss: 0.0092\n",
      "Epoch 2750, Loss: 0.0096\n",
      "Epoch 2760, Loss: 0.0120\n",
      "Epoch 2770, Loss: 0.0093\n",
      "Epoch 2780, Loss: 0.0086\n",
      "Epoch 2790, Loss: 0.0094\n",
      "Epoch 2800, Loss: 0.0089\n",
      "Epoch 2810, Loss: 0.0084\n",
      "Epoch 2820, Loss: 0.0100\n",
      "Epoch 2830, Loss: 0.0094\n",
      "Epoch 2840, Loss: 0.0093\n",
      "Epoch 2850, Loss: 0.0096\n",
      "Epoch 2860, Loss: 0.0093\n",
      "Epoch 2870, Loss: 0.0104\n",
      "Epoch 2880, Loss: 0.0086\n",
      "Epoch 2890, Loss: 0.0084\n",
      "Epoch 2900, Loss: 0.0102\n",
      "Epoch 2910, Loss: 0.0090\n",
      "Epoch 2920, Loss: 0.0083\n",
      "Epoch 2930, Loss: 0.0091\n",
      "Epoch 2940, Loss: 0.0087\n",
      "Epoch 2950, Loss: 0.0091\n",
      "Epoch 2960, Loss: 0.0089\n",
      "Epoch 2970, Loss: 0.0088\n",
      "Epoch 2980, Loss: 0.0086\n",
      "Epoch 2990, Loss: 0.0094\n",
      "Epoch 3000, Loss: 0.0086\n",
      "Epoch 3010, Loss: 0.0085\n",
      "Epoch 3020, Loss: 0.0093\n",
      "Epoch 3030, Loss: 0.0147\n",
      "Epoch 3040, Loss: 0.0091\n",
      "Epoch 3050, Loss: 0.0104\n",
      "Epoch 3060, Loss: 0.0087\n",
      "Epoch 3070, Loss: 0.0093\n",
      "Epoch 3080, Loss: 0.0088\n",
      "Epoch 3090, Loss: 0.0102\n",
      "Epoch 3100, Loss: 0.0093\n",
      "Epoch 3110, Loss: 0.0088\n",
      "Epoch 3120, Loss: 0.0083\n",
      "Epoch 3130, Loss: 0.0086\n",
      "Epoch 3140, Loss: 0.0095\n",
      "Epoch 3150, Loss: 0.0115\n",
      "Epoch 3160, Loss: 0.0088\n",
      "Epoch 3170, Loss: 0.0089\n",
      "Epoch 3180, Loss: 0.0085\n",
      "Epoch 3190, Loss: 0.0090\n",
      "Epoch 3200, Loss: 0.0102\n",
      "Epoch 3210, Loss: 0.0088\n",
      "Epoch 3220, Loss: 0.0091\n",
      "Epoch 3230, Loss: 0.0086\n",
      "Epoch 3240, Loss: 0.0089\n",
      "Epoch 3250, Loss: 0.0096\n",
      "Epoch 3260, Loss: 0.0094\n",
      "Epoch 3270, Loss: 0.0085\n",
      "Epoch 3280, Loss: 0.0091\n",
      "Epoch 3290, Loss: 0.0086\n",
      "Epoch 3300, Loss: 0.0089\n",
      "Epoch 3310, Loss: 0.0089\n",
      "Epoch 3320, Loss: 0.0092\n",
      "Epoch 3330, Loss: 0.0093\n",
      "Epoch 3340, Loss: 0.0084\n",
      "Epoch 3350, Loss: 0.0103\n",
      "Epoch 3360, Loss: 0.0087\n",
      "Epoch 3370, Loss: 0.0087\n",
      "Epoch 3380, Loss: 0.0098\n",
      "Epoch 3390, Loss: 0.0096\n",
      "Epoch 3400, Loss: 0.0085\n",
      "Epoch 3410, Loss: 0.0094\n",
      "Epoch 3420, Loss: 0.0090\n",
      "Epoch 3430, Loss: 0.0093\n",
      "Epoch 3440, Loss: 0.0089\n",
      "Epoch 3450, Loss: 0.0088\n",
      "Epoch 3460, Loss: 0.0091\n",
      "Epoch 3470, Loss: 0.0098\n",
      "Epoch 3480, Loss: 0.0084\n",
      "Epoch 3490, Loss: 0.0082\n",
      "Epoch 3500, Loss: 0.0093\n",
      "Epoch 3510, Loss: 0.0095\n",
      "Epoch 3520, Loss: 0.0086\n",
      "Epoch 3530, Loss: 0.0086\n",
      "Epoch 3540, Loss: 0.0090\n",
      "Epoch 3550, Loss: 0.0086\n",
      "Epoch 3560, Loss: 0.0086\n",
      "Epoch 3570, Loss: 0.0084\n",
      "Epoch 3580, Loss: 0.0086\n",
      "Epoch 3590, Loss: 0.0092\n",
      "Epoch 3600, Loss: 0.0098\n",
      "Epoch 3610, Loss: 0.0084\n",
      "Epoch 3620, Loss: 0.0109\n",
      "Epoch 3630, Loss: 0.0082\n",
      "Epoch 3640, Loss: 0.0100\n",
      "Epoch 3650, Loss: 0.0086\n",
      "Epoch 3660, Loss: 0.0089\n",
      "Epoch 3670, Loss: 0.0097\n",
      "Epoch 3680, Loss: 0.0094\n",
      "Epoch 3690, Loss: 0.0093\n",
      "Epoch 3700, Loss: 0.0099\n",
      "Epoch 3710, Loss: 0.0093\n",
      "Epoch 3720, Loss: 0.0087\n",
      "Epoch 3730, Loss: 0.0084\n",
      "Epoch 3740, Loss: 0.0087\n",
      "Epoch 3750, Loss: 0.0098\n",
      "Epoch 3760, Loss: 0.0084\n",
      "Epoch 3770, Loss: 0.0081\n",
      "Epoch 3780, Loss: 0.0090\n",
      "Epoch 3790, Loss: 0.0084\n",
      "Epoch 3800, Loss: 0.0085\n",
      "Epoch 3810, Loss: 0.0084\n",
      "Epoch 3820, Loss: 0.0088\n",
      "Epoch 3830, Loss: 0.0090\n",
      "Epoch 3840, Loss: 0.0087\n",
      "Epoch 3850, Loss: 0.0097\n",
      "Epoch 3860, Loss: 0.0092\n",
      "Epoch 3870, Loss: 0.0096\n",
      "Epoch 3880, Loss: 0.0093\n",
      "Epoch 3890, Loss: 0.0083\n",
      "Epoch 3900, Loss: 0.0098\n",
      "Epoch 3910, Loss: 0.0109\n",
      "Epoch 3920, Loss: 0.0091\n",
      "Epoch 3930, Loss: 0.0086\n",
      "Epoch 3940, Loss: 0.0087\n",
      "Epoch 3950, Loss: 0.0088\n",
      "Epoch 3960, Loss: 0.0086\n",
      "Epoch 3970, Loss: 0.0092\n",
      "Epoch 3980, Loss: 0.0086\n",
      "Epoch 3990, Loss: 0.0086\n",
      "Epoch 4000, Loss: 0.0082\n",
      "Epoch 4010, Loss: 0.0094\n",
      "Epoch 4020, Loss: 0.0096\n",
      "Epoch 4030, Loss: 0.0081\n",
      "Epoch 4040, Loss: 0.0098\n",
      "Epoch 4050, Loss: 0.0101\n",
      "Epoch 4060, Loss: 0.0088\n",
      "Epoch 4070, Loss: 0.0096\n",
      "Epoch 4080, Loss: 0.0097\n",
      "Epoch 4090, Loss: 0.0079\n",
      "Epoch 4100, Loss: 0.0083\n",
      "Epoch 4110, Loss: 0.0081\n",
      "Epoch 4120, Loss: 0.0114\n",
      "Epoch 4130, Loss: 0.0087\n",
      "Epoch 4140, Loss: 0.0092\n",
      "Epoch 4150, Loss: 0.0092\n",
      "Epoch 4160, Loss: 0.0090\n",
      "Epoch 4170, Loss: 0.0085\n",
      "Epoch 4180, Loss: 0.0089\n",
      "Epoch 4190, Loss: 0.0098\n",
      "Epoch 4200, Loss: 0.0086\n",
      "Epoch 4210, Loss: 0.0090\n",
      "Epoch 4220, Loss: 0.0086\n",
      "Epoch 4230, Loss: 0.0089\n",
      "Epoch 4240, Loss: 0.0082\n",
      "Epoch 4250, Loss: 0.0085\n",
      "Epoch 4260, Loss: 0.0083\n",
      "Epoch 4270, Loss: 0.0094\n",
      "Epoch 4280, Loss: 0.0091\n",
      "Epoch 4290, Loss: 0.0103\n",
      "Epoch 4300, Loss: 0.0090\n",
      "Epoch 4310, Loss: 0.0083\n",
      "Epoch 4320, Loss: 0.0091\n",
      "Epoch 4330, Loss: 0.0098\n",
      "Epoch 4340, Loss: 0.0086\n",
      "Epoch 4350, Loss: 0.0086\n",
      "Epoch 4360, Loss: 0.0085\n",
      "Epoch 4370, Loss: 0.0092\n",
      "Epoch 4380, Loss: 0.0086\n",
      "Epoch 4390, Loss: 0.0081\n",
      "Epoch 4400, Loss: 0.0098\n",
      "Epoch 4410, Loss: 0.0084\n",
      "Epoch 4420, Loss: 0.0091\n",
      "Epoch 4430, Loss: 0.0082\n",
      "Epoch 4440, Loss: 0.0084\n",
      "Epoch 4450, Loss: 0.0092\n",
      "Epoch 4460, Loss: 0.0078\n",
      "Epoch 4470, Loss: 0.0082\n",
      "Epoch 4480, Loss: 0.0092\n",
      "Epoch 4490, Loss: 0.0092\n",
      "Epoch 4500, Loss: 0.0087\n",
      "Epoch 4510, Loss: 0.0081\n",
      "Epoch 4520, Loss: 0.0098\n",
      "Epoch 4530, Loss: 0.0084\n",
      "Epoch 4540, Loss: 0.0093\n",
      "Epoch 4550, Loss: 0.0082\n",
      "Epoch 4560, Loss: 0.0083\n",
      "Epoch 4570, Loss: 0.0100\n",
      "Epoch 4580, Loss: 0.0090\n",
      "Epoch 4590, Loss: 0.0085\n",
      "Epoch 4600, Loss: 0.0086\n",
      "Epoch 4610, Loss: 0.0103\n",
      "Epoch 4620, Loss: 0.0089\n",
      "Epoch 4630, Loss: 0.0083\n",
      "Epoch 4640, Loss: 0.0092\n",
      "Epoch 4650, Loss: 0.0081\n",
      "Epoch 4660, Loss: 0.0081\n",
      "Epoch 4670, Loss: 0.0090\n",
      "Epoch 4680, Loss: 0.0085\n",
      "Epoch 4690, Loss: 0.0090\n",
      "Epoch 4700, Loss: 0.0081\n",
      "Epoch 4710, Loss: 0.0082\n",
      "Epoch 4720, Loss: 0.0088\n",
      "Epoch 4730, Loss: 0.0089\n",
      "Epoch 4740, Loss: 0.0103\n",
      "Epoch 4750, Loss: 0.0084\n",
      "Epoch 4760, Loss: 0.0086\n",
      "Epoch 4770, Loss: 0.0089\n",
      "Epoch 4780, Loss: 0.0089\n",
      "Epoch 4790, Loss: 0.0093\n",
      "Epoch 4800, Loss: 0.0083\n",
      "Epoch 4810, Loss: 0.0082\n",
      "Epoch 4820, Loss: 0.0085\n",
      "Epoch 4830, Loss: 0.0085\n",
      "Epoch 4840, Loss: 0.0083\n",
      "Epoch 4850, Loss: 0.0083\n",
      "Epoch 4860, Loss: 0.0080\n",
      "Epoch 4870, Loss: 0.0091\n",
      "Epoch 4880, Loss: 0.0091\n",
      "Epoch 4890, Loss: 0.0087\n",
      "Epoch 4900, Loss: 0.0083\n",
      "Epoch 4910, Loss: 0.0088\n",
      "Epoch 4920, Loss: 0.0100\n",
      "Epoch 4930, Loss: 0.0080\n",
      "Epoch 4940, Loss: 0.0084\n",
      "Epoch 4950, Loss: 0.0085\n",
      "Epoch 4960, Loss: 0.0085\n",
      "Epoch 4970, Loss: 0.0097\n",
      "Epoch 4980, Loss: 0.0101\n",
      "Epoch 4990, Loss: 0.0081\n",
      "Epoch 5000, Loss: 0.0080\n",
      "Epoch 5010, Loss: 0.0082\n",
      "Epoch 5020, Loss: 0.0095\n",
      "Epoch 5030, Loss: 0.0094\n",
      "Epoch 5040, Loss: 0.0083\n",
      "Epoch 5050, Loss: 0.0084\n",
      "Epoch 5060, Loss: 0.0078\n",
      "Epoch 5070, Loss: 0.0085\n",
      "Epoch 5080, Loss: 0.0086\n",
      "Epoch 5090, Loss: 0.0094\n",
      "Epoch 5100, Loss: 0.0082\n",
      "Epoch 5110, Loss: 0.0092\n",
      "Epoch 5120, Loss: 0.0107\n",
      "Epoch 5130, Loss: 0.0082\n",
      "Epoch 5140, Loss: 0.0091\n",
      "Epoch 5150, Loss: 0.0093\n",
      "Epoch 5160, Loss: 0.0091\n",
      "Epoch 5170, Loss: 0.0090\n",
      "Epoch 5180, Loss: 0.0125\n",
      "Epoch 5190, Loss: 0.0085\n",
      "Epoch 5200, Loss: 0.0087\n",
      "Epoch 5210, Loss: 0.0085\n",
      "Epoch 5220, Loss: 0.0083\n",
      "Epoch 5230, Loss: 0.0086\n",
      "Epoch 5240, Loss: 0.0090\n",
      "Epoch 5250, Loss: 0.0093\n",
      "Epoch 5260, Loss: 0.0086\n",
      "Epoch 5270, Loss: 0.0081\n",
      "Epoch 5280, Loss: 0.0086\n",
      "Epoch 5290, Loss: 0.0087\n",
      "Epoch 5300, Loss: 0.0092\n",
      "Epoch 5310, Loss: 0.0079\n",
      "Epoch 5320, Loss: 0.0082\n",
      "Epoch 5330, Loss: 0.0081\n",
      "Epoch 5340, Loss: 0.0090\n",
      "Epoch 5350, Loss: 0.0122\n",
      "Epoch 5360, Loss: 0.0083\n",
      "Epoch 5370, Loss: 0.0085\n",
      "Epoch 5380, Loss: 0.0097\n",
      "Epoch 5390, Loss: 0.0101\n",
      "Epoch 5400, Loss: 0.0090\n",
      "Epoch 5410, Loss: 0.0080\n",
      "Epoch 5420, Loss: 0.0085\n",
      "Epoch 5430, Loss: 0.0080\n",
      "Epoch 5440, Loss: 0.0081\n",
      "Epoch 5450, Loss: 0.0079\n",
      "Epoch 5460, Loss: 0.0078\n",
      "Epoch 5470, Loss: 0.0093\n",
      "Epoch 5480, Loss: 0.0089\n",
      "Epoch 5490, Loss: 0.0095\n",
      "Epoch 5500, Loss: 0.0079\n",
      "Epoch 5510, Loss: 0.0087\n",
      "Epoch 5520, Loss: 0.0084\n",
      "Epoch 5530, Loss: 0.0086\n",
      "Epoch 5540, Loss: 0.0090\n",
      "Epoch 5550, Loss: 0.0089\n",
      "Epoch 5560, Loss: 0.0082\n",
      "Epoch 5570, Loss: 0.0088\n",
      "Epoch 5580, Loss: 0.0081\n",
      "Epoch 5590, Loss: 0.0083\n",
      "Epoch 5600, Loss: 0.0086\n",
      "Epoch 5610, Loss: 0.0081\n",
      "Epoch 5620, Loss: 0.0082\n",
      "Epoch 5630, Loss: 0.0104\n",
      "Epoch 5640, Loss: 0.0087\n",
      "Epoch 5650, Loss: 0.0085\n",
      "Epoch 5660, Loss: 0.0089\n",
      "Epoch 5670, Loss: 0.0086\n",
      "Epoch 5680, Loss: 0.0082\n",
      "Epoch 5690, Loss: 0.0085\n",
      "Epoch 5700, Loss: 0.0095\n",
      "Epoch 5710, Loss: 0.0087\n",
      "Epoch 5720, Loss: 0.0090\n",
      "Epoch 5730, Loss: 0.0084\n",
      "Epoch 5740, Loss: 0.0088\n",
      "Epoch 5750, Loss: 0.0091\n",
      "Epoch 5760, Loss: 0.0093\n",
      "Epoch 5770, Loss: 0.0088\n",
      "Epoch 5780, Loss: 0.0089\n",
      "Epoch 5790, Loss: 0.0109\n",
      "Epoch 5800, Loss: 0.0083\n",
      "Epoch 5810, Loss: 0.0084\n",
      "Epoch 5820, Loss: 0.0092\n",
      "Epoch 5830, Loss: 0.0085\n",
      "Epoch 5840, Loss: 0.0104\n",
      "Epoch 5850, Loss: 0.0085\n",
      "Epoch 5860, Loss: 0.0084\n",
      "Epoch 5870, Loss: 0.0082\n",
      "Epoch 5880, Loss: 0.0080\n",
      "Epoch 5890, Loss: 0.0088\n",
      "Epoch 5900, Loss: 0.0088\n",
      "Epoch 5910, Loss: 0.0089\n",
      "Epoch 5920, Loss: 0.0090\n",
      "Epoch 5930, Loss: 0.0090\n",
      "Epoch 5940, Loss: 0.0086\n",
      "Epoch 5950, Loss: 0.0081\n",
      "Epoch 5960, Loss: 0.0082\n",
      "Epoch 5970, Loss: 0.0086\n",
      "Epoch 5980, Loss: 0.0083\n",
      "Epoch 5990, Loss: 0.0080\n",
      "Epoch 6000, Loss: 0.0091\n",
      "Epoch 6010, Loss: 0.0090\n",
      "Epoch 6020, Loss: 0.0089\n",
      "Epoch 6030, Loss: 0.0091\n",
      "Epoch 6040, Loss: 0.0085\n",
      "Epoch 6050, Loss: 0.0082\n",
      "Epoch 6060, Loss: 0.0082\n",
      "Epoch 6070, Loss: 0.0095\n",
      "Epoch 6080, Loss: 0.0082\n",
      "Epoch 6090, Loss: 0.0094\n",
      "Epoch 6100, Loss: 0.0092\n",
      "Epoch 6110, Loss: 0.0092\n",
      "Epoch 6120, Loss: 0.0092\n",
      "Epoch 6130, Loss: 0.0094\n",
      "Epoch 6140, Loss: 0.0082\n",
      "Epoch 6150, Loss: 0.0081\n",
      "Epoch 6160, Loss: 0.0092\n",
      "Epoch 6170, Loss: 0.0081\n",
      "Epoch 6180, Loss: 0.0082\n",
      "Epoch 6190, Loss: 0.0084\n",
      "Epoch 6200, Loss: 0.0095\n",
      "Epoch 6210, Loss: 0.0080\n",
      "Epoch 6220, Loss: 0.0092\n",
      "Epoch 6230, Loss: 0.0098\n",
      "Epoch 6240, Loss: 0.0100\n",
      "Epoch 6250, Loss: 0.0085\n",
      "Epoch 6260, Loss: 0.0090\n",
      "Epoch 6270, Loss: 0.0084\n",
      "Epoch 6280, Loss: 0.0078\n",
      "Epoch 6290, Loss: 0.0096\n",
      "Epoch 6300, Loss: 0.0091\n",
      "Epoch 6310, Loss: 0.0090\n",
      "Epoch 6320, Loss: 0.0089\n",
      "Epoch 6330, Loss: 0.0085\n",
      "Epoch 6340, Loss: 0.0097\n",
      "Epoch 6350, Loss: 0.0099\n",
      "Epoch 6360, Loss: 0.0093\n",
      "Epoch 6370, Loss: 0.0087\n",
      "Epoch 6380, Loss: 0.0085\n",
      "Epoch 6390, Loss: 0.0094\n",
      "Epoch 6400, Loss: 0.0081\n",
      "Epoch 6410, Loss: 0.0097\n",
      "Epoch 6420, Loss: 0.0087\n",
      "Epoch 6430, Loss: 0.0084\n",
      "Epoch 6440, Loss: 0.0086\n",
      "Epoch 6450, Loss: 0.0086\n",
      "Epoch 6460, Loss: 0.0092\n",
      "Epoch 6470, Loss: 0.0087\n",
      "Epoch 6480, Loss: 0.0085\n",
      "Epoch 6490, Loss: 0.0080\n",
      "Epoch 6500, Loss: 0.0084\n",
      "Epoch 6510, Loss: 0.0082\n",
      "Epoch 6520, Loss: 0.0082\n",
      "Epoch 6530, Loss: 0.0086\n",
      "Epoch 6540, Loss: 0.0088\n",
      "Epoch 6550, Loss: 0.0092\n",
      "Epoch 6560, Loss: 0.0082\n",
      "Epoch 6570, Loss: 0.0083\n",
      "Epoch 6580, Loss: 0.0086\n",
      "Epoch 6590, Loss: 0.0080\n",
      "Epoch 6600, Loss: 0.0086\n",
      "Epoch 6610, Loss: 0.0081\n",
      "Epoch 6620, Loss: 0.0086\n",
      "Epoch 6630, Loss: 0.0108\n",
      "Epoch 6640, Loss: 0.0110\n",
      "Epoch 6650, Loss: 0.0089\n",
      "Epoch 6660, Loss: 0.0085\n",
      "Epoch 6670, Loss: 0.0086\n",
      "Epoch 6680, Loss: 0.0087\n",
      "Epoch 6690, Loss: 0.0081\n",
      "Epoch 6700, Loss: 0.0087\n",
      "Epoch 6710, Loss: 0.0083\n",
      "Epoch 6720, Loss: 0.0085\n",
      "Epoch 6730, Loss: 0.0124\n",
      "Epoch 6740, Loss: 0.0082\n",
      "Epoch 6750, Loss: 0.0085\n",
      "Epoch 6760, Loss: 0.0093\n",
      "Epoch 6770, Loss: 0.0082\n",
      "Epoch 6780, Loss: 0.0083\n",
      "Epoch 6790, Loss: 0.0083\n",
      "Epoch 6800, Loss: 0.0082\n",
      "Epoch 6810, Loss: 0.0087\n",
      "Epoch 6820, Loss: 0.0081\n",
      "Epoch 6830, Loss: 0.0105\n",
      "Epoch 6840, Loss: 0.0090\n",
      "Epoch 6850, Loss: 0.0099\n",
      "Epoch 6860, Loss: 0.0086\n",
      "Epoch 6870, Loss: 0.0086\n",
      "Epoch 6880, Loss: 0.0090\n",
      "Epoch 6890, Loss: 0.0081\n",
      "Epoch 6900, Loss: 0.0083\n",
      "Epoch 6910, Loss: 0.0080\n",
      "Epoch 6920, Loss: 0.0088\n",
      "Epoch 6930, Loss: 0.0106\n",
      "Epoch 6940, Loss: 0.0082\n",
      "Epoch 6950, Loss: 0.0094\n",
      "Epoch 6960, Loss: 0.0101\n",
      "Epoch 6970, Loss: 0.0091\n",
      "Epoch 6980, Loss: 0.0079\n",
      "Epoch 6990, Loss: 0.0094\n",
      "Epoch 7000, Loss: 0.0087\n",
      "Epoch 7010, Loss: 0.0089\n",
      "Epoch 7020, Loss: 0.0080\n",
      "Epoch 7030, Loss: 0.0089\n",
      "Epoch 7040, Loss: 0.0085\n",
      "Epoch 7050, Loss: 0.0094\n",
      "Epoch 7060, Loss: 0.0091\n",
      "Epoch 7070, Loss: 0.0093\n",
      "Epoch 7080, Loss: 0.0079\n",
      "Epoch 7090, Loss: 0.0088\n",
      "Epoch 7100, Loss: 0.0107\n",
      "Epoch 7110, Loss: 0.0095\n",
      "Epoch 7120, Loss: 0.0093\n",
      "Epoch 7130, Loss: 0.0098\n",
      "Epoch 7140, Loss: 0.0084\n",
      "Epoch 7150, Loss: 0.0081\n",
      "Epoch 7160, Loss: 0.0094\n",
      "Epoch 7170, Loss: 0.0087\n",
      "Epoch 7180, Loss: 0.0078\n",
      "Epoch 7190, Loss: 0.0082\n",
      "Epoch 7200, Loss: 0.0083\n",
      "Epoch 7210, Loss: 0.0081\n",
      "Epoch 7220, Loss: 0.0078\n",
      "Epoch 7230, Loss: 0.0085\n",
      "Epoch 7240, Loss: 0.0079\n",
      "Epoch 7250, Loss: 0.0087\n",
      "Epoch 7260, Loss: 0.0109\n",
      "Epoch 7270, Loss: 0.0089\n",
      "Epoch 7280, Loss: 0.0086\n",
      "Epoch 7290, Loss: 0.0084\n",
      "Epoch 7300, Loss: 0.0101\n",
      "Epoch 7310, Loss: 0.0086\n",
      "Epoch 7320, Loss: 0.0082\n",
      "Epoch 7330, Loss: 0.0089\n",
      "Epoch 7340, Loss: 0.0084\n",
      "Epoch 7350, Loss: 0.0100\n",
      "Epoch 7360, Loss: 0.0085\n",
      "Epoch 7370, Loss: 0.0080\n",
      "Epoch 7380, Loss: 0.0088\n",
      "Epoch 7390, Loss: 0.0081\n",
      "Epoch 7400, Loss: 0.0080\n",
      "Epoch 7410, Loss: 0.0079\n",
      "Epoch 7420, Loss: 0.0089\n",
      "Epoch 7430, Loss: 0.0105\n",
      "Epoch 7440, Loss: 0.0085\n",
      "Epoch 7450, Loss: 0.0083\n",
      "Epoch 7460, Loss: 0.0079\n",
      "Epoch 7470, Loss: 0.0082\n",
      "Epoch 7480, Loss: 0.0080\n",
      "Epoch 7490, Loss: 0.0081\n",
      "Epoch 7500, Loss: 0.0081\n",
      "Epoch 7510, Loss: 0.0087\n",
      "Epoch 7520, Loss: 0.0083\n",
      "Epoch 7530, Loss: 0.0083\n",
      "Epoch 7540, Loss: 0.0086\n",
      "Epoch 7550, Loss: 0.0080\n",
      "Epoch 7560, Loss: 0.0086\n",
      "Epoch 7570, Loss: 0.0082\n",
      "Epoch 7580, Loss: 0.0100\n",
      "Epoch 7590, Loss: 0.0092\n",
      "Epoch 7600, Loss: 0.0083\n",
      "Epoch 7610, Loss: 0.0084\n",
      "Epoch 7620, Loss: 0.0082\n",
      "Epoch 7630, Loss: 0.0082\n",
      "Epoch 7640, Loss: 0.0095\n",
      "Epoch 7650, Loss: 0.0084\n",
      "Epoch 7660, Loss: 0.0096\n",
      "Epoch 7670, Loss: 0.0089\n",
      "Epoch 7680, Loss: 0.0081\n",
      "Epoch 7690, Loss: 0.0092\n",
      "Epoch 7700, Loss: 0.0086\n",
      "Epoch 7710, Loss: 0.0081\n",
      "Epoch 7720, Loss: 0.0104\n",
      "Epoch 7730, Loss: 0.0088\n",
      "Epoch 7740, Loss: 0.0084\n",
      "Epoch 7750, Loss: 0.0103\n",
      "Epoch 7760, Loss: 0.0084\n",
      "Epoch 7770, Loss: 0.0081\n",
      "Epoch 7780, Loss: 0.0082\n",
      "Epoch 7790, Loss: 0.0080\n",
      "Epoch 7800, Loss: 0.0082\n",
      "Epoch 7810, Loss: 0.0079\n",
      "Epoch 7820, Loss: 0.0093\n",
      "Epoch 7830, Loss: 0.0080\n",
      "Epoch 7840, Loss: 0.0079\n",
      "Epoch 7850, Loss: 0.0081\n",
      "Epoch 7860, Loss: 0.0102\n",
      "Epoch 7870, Loss: 0.0092\n",
      "Epoch 7880, Loss: 0.0079\n",
      "Epoch 7890, Loss: 0.0093\n",
      "Epoch 7900, Loss: 0.0081\n",
      "Epoch 7910, Loss: 0.0094\n",
      "Epoch 7920, Loss: 0.0082\n",
      "Epoch 7930, Loss: 0.0086\n",
      "Epoch 7940, Loss: 0.0080\n",
      "Epoch 7950, Loss: 0.0087\n",
      "Epoch 7960, Loss: 0.0090\n",
      "Epoch 7970, Loss: 0.0099\n",
      "Epoch 7980, Loss: 0.0082\n",
      "Epoch 7990, Loss: 0.0083\n",
      "Epoch 8000, Loss: 0.0088\n",
      "Epoch 8010, Loss: 0.0086\n",
      "Epoch 8020, Loss: 0.0136\n",
      "Epoch 8030, Loss: 0.0082\n",
      "Epoch 8040, Loss: 0.0084\n",
      "Epoch 8050, Loss: 0.0106\n",
      "Epoch 8060, Loss: 0.0079\n",
      "Epoch 8070, Loss: 0.0086\n",
      "Epoch 8080, Loss: 0.0080\n",
      "Epoch 8090, Loss: 0.0084\n",
      "Epoch 8100, Loss: 0.0088\n",
      "Epoch 8110, Loss: 0.0082\n",
      "Epoch 8120, Loss: 0.0085\n",
      "Epoch 8130, Loss: 0.0083\n",
      "Epoch 8140, Loss: 0.0081\n",
      "Epoch 8150, Loss: 0.0092\n",
      "Epoch 8160, Loss: 0.0082\n",
      "Epoch 8170, Loss: 0.0088\n",
      "Epoch 8180, Loss: 0.0104\n",
      "Epoch 8190, Loss: 0.0095\n",
      "Epoch 8200, Loss: 0.0080\n",
      "Epoch 8210, Loss: 0.0081\n",
      "Epoch 8220, Loss: 0.0078\n",
      "Epoch 8230, Loss: 0.0086\n",
      "Epoch 8240, Loss: 0.0093\n",
      "Epoch 8250, Loss: 0.0078\n",
      "Epoch 8260, Loss: 0.0080\n",
      "Epoch 8270, Loss: 0.0083\n",
      "Epoch 8280, Loss: 0.0085\n",
      "Epoch 8290, Loss: 0.0087\n",
      "Epoch 8300, Loss: 0.0084\n",
      "Epoch 8310, Loss: 0.0088\n",
      "Epoch 8320, Loss: 0.0083\n",
      "Epoch 8330, Loss: 0.0080\n",
      "Epoch 8340, Loss: 0.0082\n",
      "Epoch 8350, Loss: 0.0081\n",
      "Epoch 8360, Loss: 0.0100\n",
      "Epoch 8370, Loss: 0.0089\n",
      "Epoch 8380, Loss: 0.0086\n",
      "Epoch 8390, Loss: 0.0080\n",
      "Epoch 8400, Loss: 0.0079\n",
      "Epoch 8410, Loss: 0.0085\n",
      "Epoch 8420, Loss: 0.0087\n",
      "Epoch 8430, Loss: 0.0080\n",
      "Epoch 8440, Loss: 0.0088\n",
      "Epoch 8450, Loss: 0.0081\n",
      "Epoch 8460, Loss: 0.0100\n",
      "Epoch 8470, Loss: 0.0113\n",
      "Epoch 8480, Loss: 0.0085\n",
      "Epoch 8490, Loss: 0.0083\n",
      "Epoch 8500, Loss: 0.0081\n",
      "Epoch 8510, Loss: 0.0078\n",
      "Epoch 8520, Loss: 0.0083\n",
      "Epoch 8530, Loss: 0.0094\n",
      "Epoch 8540, Loss: 0.0085\n",
      "Epoch 8550, Loss: 0.0090\n",
      "Epoch 8560, Loss: 0.0105\n",
      "Epoch 8570, Loss: 0.0082\n",
      "Epoch 8580, Loss: 0.0084\n",
      "Epoch 8590, Loss: 0.0087\n",
      "Epoch 8600, Loss: 0.0095\n",
      "Epoch 8610, Loss: 0.0085\n",
      "Epoch 8620, Loss: 0.0084\n",
      "Epoch 8630, Loss: 0.0082\n",
      "Epoch 8640, Loss: 0.0082\n",
      "Epoch 8650, Loss: 0.0083\n",
      "Epoch 8660, Loss: 0.0080\n",
      "Epoch 8670, Loss: 0.0083\n",
      "Epoch 8680, Loss: 0.0082\n",
      "Epoch 8690, Loss: 0.0080\n",
      "Epoch 8700, Loss: 0.0094\n",
      "Epoch 8710, Loss: 0.0082\n",
      "Epoch 8720, Loss: 0.0090\n",
      "Epoch 8730, Loss: 0.0099\n",
      "Epoch 8740, Loss: 0.0086\n",
      "Epoch 8750, Loss: 0.0089\n",
      "Epoch 8760, Loss: 0.0079\n",
      "Epoch 8770, Loss: 0.0086\n",
      "Epoch 8780, Loss: 0.0085\n",
      "Epoch 8790, Loss: 0.0080\n",
      "Epoch 8800, Loss: 0.0090\n",
      "Epoch 8810, Loss: 0.0086\n",
      "Epoch 8820, Loss: 0.0110\n",
      "Epoch 8830, Loss: 0.0082\n",
      "Epoch 8840, Loss: 0.0090\n",
      "Epoch 8850, Loss: 0.0088\n",
      "Epoch 8860, Loss: 0.0082\n",
      "Epoch 8870, Loss: 0.0087\n",
      "Epoch 8880, Loss: 0.0081\n",
      "Epoch 8890, Loss: 0.0081\n",
      "Epoch 8900, Loss: 0.0082\n",
      "Epoch 8910, Loss: 0.0091\n",
      "Epoch 8920, Loss: 0.0087\n",
      "Epoch 8930, Loss: 0.0090\n",
      "Epoch 8940, Loss: 0.0086\n",
      "Epoch 8950, Loss: 0.0085\n",
      "Epoch 8960, Loss: 0.0082\n",
      "Epoch 8970, Loss: 0.0089\n",
      "Epoch 8980, Loss: 0.0080\n",
      "Epoch 8990, Loss: 0.0081\n",
      "Epoch 9000, Loss: 0.0082\n",
      "Epoch 9010, Loss: 0.0094\n",
      "Epoch 9020, Loss: 0.0081\n",
      "Epoch 9030, Loss: 0.0079\n",
      "Epoch 9040, Loss: 0.0081\n",
      "Epoch 9050, Loss: 0.0092\n",
      "Epoch 9060, Loss: 0.0084\n",
      "Epoch 9070, Loss: 0.0085\n",
      "Epoch 9080, Loss: 0.0089\n",
      "Epoch 9090, Loss: 0.0083\n",
      "Epoch 9100, Loss: 0.0093\n",
      "Epoch 9110, Loss: 0.0082\n",
      "Epoch 9120, Loss: 0.0093\n",
      "Epoch 9130, Loss: 0.0106\n",
      "Epoch 9140, Loss: 0.0089\n",
      "Epoch 9150, Loss: 0.0094\n",
      "Epoch 9160, Loss: 0.0087\n",
      "Epoch 9170, Loss: 0.0084\n",
      "Epoch 9180, Loss: 0.0098\n",
      "Epoch 9190, Loss: 0.0084\n",
      "Epoch 9200, Loss: 0.0092\n",
      "Epoch 9210, Loss: 0.0079\n",
      "Epoch 9220, Loss: 0.0083\n",
      "Epoch 9230, Loss: 0.0080\n",
      "Epoch 9240, Loss: 0.0089\n",
      "Epoch 9250, Loss: 0.0093\n",
      "Epoch 9260, Loss: 0.0088\n",
      "Epoch 9270, Loss: 0.0097\n",
      "Epoch 9280, Loss: 0.0084\n",
      "Epoch 9290, Loss: 0.0081\n",
      "Epoch 9300, Loss: 0.0102\n",
      "Epoch 9310, Loss: 0.0083\n",
      "Epoch 9320, Loss: 0.0089\n",
      "Epoch 9330, Loss: 0.0089\n",
      "Epoch 9340, Loss: 0.0084\n",
      "Epoch 9350, Loss: 0.0089\n",
      "Epoch 9360, Loss: 0.0085\n",
      "Epoch 9370, Loss: 0.0078\n",
      "Epoch 9380, Loss: 0.0081\n",
      "Epoch 9390, Loss: 0.0098\n",
      "Epoch 9400, Loss: 0.0081\n",
      "Epoch 9410, Loss: 0.0087\n",
      "Epoch 9420, Loss: 0.0082\n",
      "Epoch 9430, Loss: 0.0081\n",
      "Epoch 9440, Loss: 0.0086\n",
      "Epoch 9450, Loss: 0.0090\n",
      "Epoch 9460, Loss: 0.0079\n",
      "Epoch 9470, Loss: 0.0089\n",
      "Epoch 9480, Loss: 0.0100\n",
      "Epoch 9490, Loss: 0.0094\n",
      "Epoch 9500, Loss: 0.0087\n",
      "Epoch 9510, Loss: 0.0082\n",
      "Epoch 9520, Loss: 0.0101\n",
      "Epoch 9530, Loss: 0.0094\n",
      "Epoch 9540, Loss: 0.0090\n",
      "Epoch 9550, Loss: 0.0092\n",
      "Epoch 9560, Loss: 0.0088\n",
      "Epoch 9570, Loss: 0.0085\n",
      "Epoch 9580, Loss: 0.0093\n",
      "Epoch 9590, Loss: 0.0087\n",
      "Epoch 9600, Loss: 0.0132\n",
      "Epoch 9610, Loss: 0.0081\n",
      "Epoch 9620, Loss: 0.0080\n",
      "Epoch 9630, Loss: 0.0078\n",
      "Epoch 9640, Loss: 0.0091\n",
      "Epoch 9650, Loss: 0.0093\n",
      "Epoch 9660, Loss: 0.0084\n",
      "Epoch 9670, Loss: 0.0087\n",
      "Epoch 9680, Loss: 0.0090\n",
      "Epoch 9690, Loss: 0.0081\n",
      "Epoch 9700, Loss: 0.0090\n",
      "Epoch 9710, Loss: 0.0084\n",
      "Epoch 9720, Loss: 0.0080\n",
      "Epoch 9730, Loss: 0.0085\n",
      "Epoch 9740, Loss: 0.0096\n",
      "Epoch 9750, Loss: 0.0085\n",
      "Epoch 9760, Loss: 0.0098\n",
      "Epoch 9770, Loss: 0.0083\n",
      "Epoch 9780, Loss: 0.0079\n",
      "Epoch 9790, Loss: 0.0084\n",
      "Epoch 9800, Loss: 0.0084\n",
      "Epoch 9810, Loss: 0.0083\n",
      "Epoch 9820, Loss: 0.0085\n",
      "Epoch 9830, Loss: 0.0080\n",
      "Epoch 9840, Loss: 0.0081\n",
      "Epoch 9850, Loss: 0.0081\n",
      "Epoch 9860, Loss: 0.0081\n",
      "Epoch 9870, Loss: 0.0083\n",
      "Epoch 9880, Loss: 0.0090\n",
      "Epoch 9890, Loss: 0.0085\n",
      "Epoch 9900, Loss: 0.0095\n",
      "Epoch 9910, Loss: 0.0076\n",
      "Epoch 9920, Loss: 0.0090\n",
      "Epoch 9930, Loss: 0.0082\n",
      "Epoch 9940, Loss: 0.0084\n",
      "Epoch 9950, Loss: 0.0082\n",
      "Epoch 9960, Loss: 0.0085\n",
      "Epoch 9970, Loss: 0.0082\n",
      "Epoch 9980, Loss: 0.0084\n",
      "Epoch 9990, Loss: 0.0087\n",
      "ÏµúÏ¢Ö Loss: 0.0083\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "import numpy as np\n",
    "\n",
    "# Îç∞Ïù¥ÌÑ∞ Î°úÎìú\n",
    "diabetes = load_diabetes()\n",
    "X = diabetes.data  # 442Í∞ú ÏÉòÌîå, 10Í∞úÏùò feature\n",
    "y = diabetes.target.reshape(-1, 1)  # ÌÉÄÍ≤üÍ∞í (Ïó∞ÏÜçÌòï ÌöåÍ∑Ä Í∞í)\n",
    "\n",
    "# ÌëúÏ§ÄÌôî (Ï†ïÍ∑úÌôî)\n",
    "X = (X - X.mean(axis=0)) / X.std(axis=0)\n",
    "y = y / y.max()  # Ï†ïÎãµÍ∞íÎèÑ 0~1Î°ú Ïä§ÏºÄÏùºÎßÅ\n",
    "\n",
    "model = NeuralNetwork(\n",
    "    layers=[\n",
    "        Linear(10, 32),  # 10Í∞ú feature -> Ï§ëÍ∞Ñ Î†àÏù¥Ïñ¥ 32Í∞ú ÎÖ∏Îìú\n",
    "        ReLU(),\n",
    "        Linear(32, 1)    # Ï∂úÎ†• 1Í∞ú\n",
    "    ],\n",
    "    optimizer=Adam(lr=0.01)\n",
    ")\n",
    "\n",
    "# ÌïôÏäµ\n",
    "model.train(X, y, epochs=10000, batch_size=32)\n",
    "# ÏµúÏ¢Ö ÏòàÏ∏° Î∞è ÏµúÏ¢Ö Loss Ï∂úÎ†•\n",
    "y_pred = model.forward(X)\n",
    "final_loss = MSE.loss(y, y_pred)\n",
    "print(f\"ÏµúÏ¢Ö Loss: {final_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
